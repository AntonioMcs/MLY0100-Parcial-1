{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 ‚Äî Comparativa de Modelos\n",
        "\n",
        "**Proyecto:** MLY0100 ‚Äî Pipeline Diabetes con Kedro  \n",
        "**Autor:** Antonio Sep√∫lveda  \n",
        "**Fecha:** 2025\n",
        "\n",
        "---\n",
        "Este notebook eval√∫a **m√∫ltiples modelos de clasificaci√≥n** para determinar cu√°l ofrece el mejor desempe√±o en la predicci√≥n de **diabetes**.\n",
        "\n",
        "Se realiza la comparaci√≥n usando:\n",
        "- RandomForest (modelo actual del pipeline)\n",
        "- Logistic Regression\n",
        "- Support Vector Machine (SVM)\n",
        "- KNN\n",
        "- Gradient Boosting\n",
        "\n",
        "Finalmente, se elige el **mejor modelo**, validando m√©tricas y visualizaciones adicionales."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìö 1. Importaci√≥n de Librer√≠as"
      ]
    },

    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìÅ 2. Carga del Dataset Limpio\n",
        "\n",
        "Se cargan los datos ya limpios desde el pipeline Kedro:"
      ]
    },

    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../data/02_intermediate/diabetes_cleaned.csv\")\n",
        "df.head()"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ 3. Preparaci√≥n de Variables\n",
        "\n",
        "Separaci√≥n entre variables predictoras **X** y variable objetivo **y (Outcome)**."
      ]
    },

    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "X = df.drop(\"Outcome\", axis=1)\n",
        "y = df[\"Outcome\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ 4. Definici√≥n de Modelos a Comparar"
      ]
    },

    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "modelos = {\n",
        "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=500),\n",
        "    \"SVM\": SVC(kernel=\"rbf\", probability=True),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42)\n",
        "}"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä 5. Entrenamiento y Evaluaci√≥n"
      ]
    },

    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "resultados = []\n",
        "\n",
        "for nombre, modelo in modelos.items():\n",
        "    modelo.fit(X_train, y_train)\n",
        "    pred = modelo.predict(X_test)\n",
        "\n",
        "    resultados.append({\n",
        "        \"Modelo\": nombre,\n",
        "        \"Accuracy\": accuracy_score(y_test, pred),\n",
        "        \"Precision\": precision_score(y_test, pred),\n",
        "        \"Recall\": recall_score(y_test, pred),\n",
        "        \"F1-score\": f1_score(y_test, pred)\n",
        "    })\n",
        "\n",
        "df_resultados = pd.DataFrame(resultados)\n",
        "df_resultados"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä 6. Comparaci√≥n Gr√°fica de M√©tricas"
      ]
    },

    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=df_resultados.melt(id_vars=\"Modelo\"), x=\"variable\", y=\"value\", hue=\"Modelo\")\n",
        "plt.title(\"Comparaci√≥n de M√©tricas entre Modelos\", fontsize=14)\n",
        "plt.xlabel(\"M√©trica\")\n",
        "plt.ylabel(\"Valor\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèÜ 7. Selecci√≥n Autom√°tica del Mejor Modelo"
      ]
    },

    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "mejor = df_resultados.sort_values(by=\"Accuracy\", ascending=False).iloc[0]\n",
        "mejor"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El **mejor modelo** seg√∫n Accuracy es:"
      ]
    },

    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(f\"üèÜ Mejor Modelo: {mejor['Modelo']} con Accuracy = {mejor['Accuracy']:.4f}\")"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç 8. Matriz de Confusi√≥n del Mejor Modelo"
      ]
    },

    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "modelo_final = modelos[mejor['Modelo']]\n",
        "pred_final = modelo_final.predict(X_test)\n",
        "cm = confusion_matrix(y_test, pred_final)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(f\"Matriz de Confusi√≥n ‚Äî {mejor['Modelo']}\")\n",
        "plt.xlabel(\"Predicho\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.show()"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìù 9. Conclusiones\n",
        "\n",
        "- Se compararon 5 modelos de clasificaci√≥n.\n",
        "- El modelo con mayor rendimiento fue **{mejor['Modelo']}**, con un Accuracy de **{mejor['Accuracy']:.4f}**.\n",
        "- Este notebook permite validar si el modelo utilizado en Kedro es efectivamente el m√°s √≥ptimo.\n",
        "- Si un modelo alternativo supera al actual, puede integrarse f√°cilmente al pipeline.\n"
      ]
    }
  ],

  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },

  "nbformat": 4,
  "nbformat_minor": 5
}
